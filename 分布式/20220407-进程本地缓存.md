

**为什么要用缓存**

从本质上来说，如果你想到了使用功能缓存，那么基本上是系统面临了几方面的需求

**1、复用计算资源：**把需要重复计算和执行的业务结果缓存起来、一次执行，多次复用可以节省大量的计算机资源。

**2、缓解IO压力:** 不同的设备可以承载的IO吞吐量有很大差别，比如磁盘、和内存的IO吞吐量就相差上百倍，通过缓存的方式，把经常访问的数据保存到IO速度更快的设备上，不仅可以提升程序的性能，还能缓解低速IO设备的压力。

**3、缓解网络压力:** 数据经过网络后会有网络传输、延时的成本，那么我们可以把常用的数据放到离自己最近的地方（比如CDN）、或者直接放到客户端本地（比如HTTP缓存），这样就可以减少或者直接避免网络的传输，通过这种方式不仅可以以缓解网络压力，还可提升访问速度，节省客户端的流量。

  

  

## **选择缓存需要考虑的几个方面**


如果你有了用缓存的需求，决定要选择一款缓存引入到项目中使用，那么如何衡量一个缓存是否满足你的要求需要考虑到几个方面的问题。

### **1、吞吐量问题：**

缓存通常是作为流量冲击的第一道栅栏，显然需要一个吞吐量更高的缓存才可以抗住更大的流量冲击。

  

### **2、缓存命中率问题**

大部分缓存都是把数据放到内存中的，而内存资源是有限的，那意味着我们必须把最有价值（访问频率高的）数据保存到缓存中，所以当缓存数据超过内存资源的限制时，我们如何把那些不经常访问的资源淘汰出去，把那些有价值的数据留在缓存中，保证经常访问的资源都直接可以从缓存获取到，而缓存命中率就是缓存利用率的一个客观体现，而缓存命中率的高低是则需要关注缓存使用了什么机制的缓存淘汰策略所决定的。

  

### **3、缓存扩展功能**

缓存除了吞吐量和命中率，另外一些扩展的功能也是开发人员比较关注的，比如缓存有没有超时失效的机制（设置过期时间，过期缓存就删除）、是否支持事件订阅（缓存删除或失效后会触发事件订阅）。

  

### **4、缓存共享问题**

进程内的缓存只能在单个应用内使用，没有办法多个进程共享一份缓存，所以每个进程都需要缓存同样的资源，这样对于资源来说是一种浪费，而且不同进程的缓存的数据和失效时间可能不一样，这样就很可能造成多个进程之间缓存不一致问题。 另外一方面应用缓存也受限于对应的开发语言，不同的开发语言的程序之间缓存无法进行共享。 所以解决这些问题就需要一种可以独立于应用和开发语言之外的缓存，也就是我们现在常用的分布式缓存。

  

  

## **JAVA里常用的应用级缓存**


这里我们首先了解一下应用级缓存（本地缓存），在JAVA里一般我们有几种类型的应用缓存可以选择，比如JAVA本身提供的缓存（ConcurrentHashMap)、还有第三方提供的应用缓存Ecache、 **Guava** 、caffeine。

因为JAVA提供的缓存只提供了简单的缓存增加删除机制，基本上没有任何扩展的功能，所以就衍生了一批第三方的应用缓存，第三方应用缓存提供了丰富的扩展功能，比如说我们上面的缓存自动失效机制、缓存事件订阅机制，这些扩展机制能给开发人员管理缓存提供很大的便捷性。 。

  

### **吞吐量对比**

一个缓存首先必须要保证的就是并发读写的安全性，所以缓存吞吐量第一个制约因素就是缓存容器是如何处理并发读写的。

从JAVA内部提供的HashMap说起，因为HashMap并不能保证并发下的读写安全性，所以它并不适合做缓存，为了保证并发读写时数据的安全性，常用的方式就是进行加锁，所以就有了HashTable 这样的容器，但HashTable 比较简单粗暴，对读写方法都加上了synchronized来进行加锁， 这种方式读写会对整个容器进行加锁，所以并发的性能并不理想。

HashTable 的锁影响的是整个容器，所以必须减少锁的影响范围，然后就有了ConcurrentHashMap这样的容器,ConcurrentHashMap初期版本采用分段的形式进行加锁， 分段锁的方式只会对其中一部分数据加锁，这样显著的提升了并发的性能，不过在JKD8后ConcurrentHashMap又进行了进一步的升级，只会对链表的头部节点进行加锁，这样的话加锁的粒度就缩小到了只锁定一个链表，加锁的粒度更小，性能有了进一步的提升。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1147-O8w3L6.jpg)

  

  

ConcurrentHashMap的性能已经得到了很好的优化，而第三方缓存也没有打算在这方面重复造轮子，所以基本上第三方缓存都是基于ConcurrentHashMap做扩展的， 他们的区别在ConcurrentHashMap的基础上如何去做扩展功能，而这些处理的机制不同也就造就了各自性能的差异。

第三方缓存为了实现这些扩展功能，所以必须在ConcurrentHashMap 上做一些额外的操作，比如读取数据的时候要记录一些读取行为，以便于后期作为缓存淘汰缓存数据的分析。 在写入数据的时候同时也要记录一些信息，根据对应的信息触发后期的事件。而记录这些数据的操作就成了一个性能负担了，Ecache和Guava都是在数据写入和读取的时候都是同步记录这些附加信息的，因为容器本身读写都需要进行加锁，而这些附加操作又是在加锁的操作内的，所以就必然会降低容器本身的性能。

在cafeine中采用了异步的模式，读写记录的额外信息都是采用异步的方式写入到日志的，这样就尽可能的减少了记录附加信息对容器本身的影响。 在读ConcurrentHashMap读写数据的时候并不会马上写入这些附加信息，而是会把这些信息放到一个环形缓冲区中再异步的执行，缓存区设置了一个固定的大小，在写入数据的缓冲区满了，那么就必须同步写入。在读取数据的时候超过缓冲区的读取数据就会直接丢弃，因为读取数据的记录是为了做一些统计分析用的，所以丢失的这部分附加信息是可以接受的。

  

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1147-Y9FxgT.jpg)

  

  

### **命中率的衡量**

除了缓存本身的性能之外，还有需要考虑的关键因素就是缓存的命中率，缓存资源是有限的，如何把应用最常用的信息保留在缓存中，淘汰那些不会被系统访问到的信息呢，这就是缓存所使用的淘汰策略所决定。

  

### **FIFO 策略**

FIFO策略的思想是，基于保存到缓存的先后顺序来评估数据的价值，越往最后进入缓存的数据在将来也越有可能会被访问。基于这种思想，FIFO 会把保存的数据按顺序的放到一个队列里，队列头部是最新进入缓存的，队列尾部就最后进入缓存的，在缓存淘汰的时候最优先淘汰是那些队列尾部的最早进入缓存的数据。

FIFO策略很简单，但这种机制有着明显的问题，根据进入缓存的先后顺序来判定缓存以后的访问频率并不合理，相反很多时候越早缓存的数据可能越重要，比如说数据预热的时候,通常在最开始就会把数据初始到缓存里面。

  

### **LRU 策略**

LRU的思想是，基于最近的访问时间来评估数据的价值，如果一个数据最近时间访问过那么它在将来也越有可能被访问。LRU会在缓存数据被访问的时候同时更新此数据的访问时间，根据时间的先后顺序进行排列，进行缓存淘汰时，会优先把那些很久都没有访问过的数据淘汰掉。

**遗留问题：**LRU的缓存淘汰策略比较符合大多数场景， 不过无法避免热点数据问题，有些缓存数据可能只在某一时间段访问频率高（比如说早中晚不同的时间段热点商品不一样），平常并不会经常访问，一旦到对应的时间点就会被大量使用，而使用LRU策略的话就会把这部分的数据淘汰掉。

  

### **LFU策略**

LFU的思想是，基于数据历史被访问的次数来评估数据的价值，如果一个数据被访问的次数越多的那么在将来也可能会经常被使用。LFU会统计为每一个缓存维护一个计数器，当某个缓存被访问时，其被访问的次数就会累加。 在进行缓存淘汰的时候会优先淘汰哪些访问次数较少的缓存数据。

**遗留问题：**

1、LFU有效的解决了LRU的热点数据缓存的问题，不过LFU策略中要为每个缓存key维护一个计数器，如果Key的数量特别多，那么维护每个缓存的计数器也需要大量的内存资源。而这部分工作会给缓存应用增加了很大的负担。

2、另外LFU 无法识别偶发性的流量，比如突然来了一个病毒，导致一种药品需求量大增，那么此药品数据的访问次数就会大增，但是一旦这个病毒消失了，那么可能药品数据就很少再用到了，而由于该缓存统计的访问次数太多，导致这类缓存后期无法淘汰。

3、相对的，有些数据是因为偶发性导致访问次数偏多，那么还有一部分数据却是经常访问，但是频率并不是特别高，那么这一部分数据虽然经常要用但是由于访问次数相对不是很多，所以完全基于访问次数淘汰的时候会把这一部分数据淘汰掉。

  

### **TinyLFU 策略。**

为了减少维护计数器带来的额外存储负担，TinyLFU 在LFU的基础上对计数器的功能进行了优化，TinyLFU 使用了一种Count–Min Sketch 算法，其核心思想和布隆过滤器如出一辙，它们都是想用最小的资源代价来记录最多的数据。

Sketch不会保存每个缓存key的计数器，而是在内存中创建一个一定长度的位图来记录某个哈希值被访问的次数。对照下面的图，假如我们创建了了一个长度为14的位图，现在我们需要把数据key 1和key 2的访问次数记录到位图里去，首先会根据多个哈希函数计算出Key的不同的哈希值，然后用哈希值对位图的长度进行取模，最后得到位图的下标位，然后每访问一次key都将对应位图下标数据的累计值加1。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1147-zZabZ9.jpg)

  

同时在获取某个KEY的访问次数，会以最小的那个统计值为准。通过这种方式可以避免为每个缓存key都维护一个计数器，可以在LFU的基础上节省大量的内存资源。

  

### **W-TinyLFU 策略**

W-TinyLFU 在 TinyLFU 的基础上进一步优化解决了LFU的另外两个问题。

首先 W-TinyLFU 通过衰减策略解决了偶发性流量问题，偶发性流量的特点就是短时间的访问次数特别高，而一旦过了这个热点，以后就可能很少再访问了。 衰减策略的核心就是每当过一段时间或者统计值达到某个数值，就会对原来缓存key的统计值进行减半，如果缓存数据只是某一个时间访问次数很高，后面很少访问的话，那么经过多次减半之后，对应缓存的key 统计数值就会越来越小，从而最后淘汰掉。

另外一方面，对于那些经常访问，但是访问次数并不是特别多的数据，W-TinyLFU 综合了LRU的策略，用额外的一小部分的空间来使用LRU策略来记录这些时间上比较活跃的缓存数据，从而避免了这部分的缓存数据被淘汰。

  

  

### **各种本地缓存综合对比**

最后综合吞吐量、缓存淘汰策略、扩展机制、分布式支持来对比一下java 各种应用缓存指标

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1147-kDE7Ao.jpg)

  

> 本文转自 [https://zhuanlan.zhihu.com/p/461795493](https://zhuanlan.zhihu.com/p/461795493)，如有侵权，请联系删除。