

volatile作为并发问题不得不谈的一个关键字，在代码层面简简单单的一个关键字却解决了很大一部分的并发程序问题，我们常常遇到并发问题就会想到使用synchronized和volatile，有时候甚至到了胡乱用的程度，因为我们压根就不知道使用volatile解决的到底是什么问题，要究其根本还是我们对于产生并发问题的核心不了解，所以为了更清楚的知道这些关键字后面所代表的意义，那么就有必要深挖一下这些关键字背后到底做了什么操作，解决了什么问题。

首先这里我们将从volatile解决了哪两个方面的问题，然后我们下面针对这两个方向来逐个击破这些问题的起源以及解决方案，最后又是如何体现在volatile关键字里的。

从全局上来说，并发层面的问题主要包含三个，一是CPU切换指令执行导致的原子性问题，二是CPU缓存导致的可见性问题，三是编译器和操作系统进行指令优化导致的指令重排序问题。在代码层面volatile是针对指令重排序问题和缓存可见性问题的解决方案，下面我们来逐个说明它是如何解决的。

  

## **一、解决了指令重排序问题。**






### **什么是重排序**

首先我们理解一下重排序、针对程序代码语而言，编译器可以对程序指令进行重排序，但是所有的重排序都需要遵循as-if-serial 语义原则，as-if-serial 的意思是说，可以允许编译器和处理器进行重排序，但是有一个条件，就是不管怎么重排序都不能改变单线程执行程序的结果，概念很难一下子明白那我们就拿代码来说。

```java
 int a=1;    //语句1
 int b=2;    //语句2
 int c=a+1;  //语句3
```

上面语句2和语句1、3之间没有任何依赖关系，而语句1和语句3却有着明确的依赖关系，遇到这样的语句（换成指令也一样）编译器就可以先执行语句2再执行语句1、3，因为这样调整顺序后对程序结果是没有任何影响的，所以这里就允许把语句2调整到最开始或者最后执行都行。

反之编译器是不能把语句3重排序到语句1之前的，因为语句3的运行必须依赖于语句1的结果，它们是有数据依赖关系的，如果调整了语句3的排序就有可能影响到最终的程序运行结果，这也就是as-if-serial语义所表达的，只要程序结果不会改变，那么无论你怎么调整语句或指令的顺序，都没事，因为从结果上来看我好像就是完全串行按顺序的把代码从头执行到尾。

  

### **为什么重排序**

这里读者肯定已经想到了，重排序最终也不过是为了能提升计算机的性能，这里的性能大多数情况下都是提升CPU的利用率，指令重排序说得挺晦涩的，不过种思想在生活中处处可见，通俗来讲就是在做一些流程任务的时候，我们会调整一些流程的顺序，以提高我们执行任务的效率，从而节省时间。

比如说你在做饭的时候，为什么最先开始是煮饭呢，因为煮饭的时间最长，而且只要我们淘米按上电饭煲按钮后就只能等待了，在这里煮饭其实就是在进行IO，而淘米和按下电饭煲的开关就是CPU在发送指令。 CPU发送指令完之后就可以马上去干别的事情了，比如说洗菜，收拾桌子，这样CPU也没闲着，煮饭的这段时间也没浪费，整体时间节省下来了。但是如果你先把菜都做完再来煮饭，那你饭没熟是没办法开餐的，所以CPU就在这里干等着 浪费时间，于是CPU资源浪费了，然后整体吃饭的时间又拖长了。

  

  

### **重排序带来的问题**

单线程的重排序很简单，因为可以通过语义分析就能知道前后代码的依赖性，但是多线程就不一样了，多线程环境里编译器和CPU指令优化根本无法识别多个线程之间存在的数据依赖性，比如说下面的程序代码如果两个方法在l两个不同的线程里面调用就可能出现问题。

```java
  private static int value;
     private static boolean flag;
 
     public static  void  init(){
         value=8;     //语句1
         flag=true;  //语句2
     }
 
     public static void getValue(){
         if(flag){
             System.out.println(value);
         }
     }
```

根据上面代码，如果程序代码运行都是按顺序的，那么getValue() 中打印的value值必定是等于8的，但是如果init(）方法经过了指令重排序，那么结果就不一定了。根据as-if-serial 原则，init()方法是允许进行指令重排序，因为语句1和语句2之间没有依赖关系。 进行重排序后代码执行顺序可能如下。

```java
  flag=true;  //语句2  
  value=8;     //语句1
```

如果init()方法经过了指令重排序后，这个时候两个线程分别调用 init()和getValue()方法，那么就有可能出现下图的情况，导致最终打印出来的value数据等于0。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1213-r6xFqF.jpg)

  

  

### **禁止指令重排序。**

很明显在多线程的环境下，指令重排序就很容易出现问题，导致程序BUG，而通常这种情况下编译器和处理器是根本无法通过语义分析来知道代码指令的依赖关系的。

所以这个问题只有写代码的人才能清楚这其中的逻辑依赖关系，如果发现有些地方不能进行指令重排序，那么编写代码的人员就必须用一种指令来告诉编译器或处理器哪些地方是存在逻辑依赖的，这些地方不能进行重排序。

当然既然有需求了，必然也就有了解决方案，解决这个问题在编译器层面 和CPU层面都提供了一套内存屏障指令来禁止重排序的指令。不过令开发人员比较崩溃的是，不同的操作系统的内存屏障的指令和语义是不一样的。要全部都了解，显然对软件开发人员来说有点太苛刻了，所以开发人员就有了新的需求，谁能把这些都兼容了，就给我们一套统一的指令就行了。

Java想要壮大，毕竟还是需要靠开发人员的，所以JAVA开发人员就是JVM程序员的上帝了，为了简化Java开发人员的工作，所以，JVM人员封装了一套规范，把这些复杂的指令操作与开发人员隔离开来，让程序员不需要关心这些系统层面的操作指令，而这套规范就是我们常说的**Java内存模型(JMM)**，JMM定义了几个happens before原则来指导并发程序编写的正确性。JMM在结果层面告诉了程序开发人员使用什么方法能保证程序在多线程环境下的正确性，同时也是向底层的JVM开发人员定义了一套约定，JVM的实现必须遵守这些约定。

好了，到这里文章的主角Volatile终于上场了，在JMM层面定义了，操作Volatile关键字修饰的变量的时候，是不允许进行指令重排序的，那么原因就是Volatile遵循了JMM规范，而底层原理就是Volatile通过内存屏障禁止了指令的重排序。

  

  

  

### **使用volatile解决重排序经典案例**

最后再结合一个经典的场景来体验一下volatile的功效。单例模式中在“使用了双重检查锁”之后，在极端的情况下会出现拿到一个未初始化完成的对象的问题，其实这个问题就是指令重排序导致的问题，当然解决方案我们大家也都已经知道了，就是用volatile来修改定义的对象即可，因为volatile可以禁止指令重排序。

```java
 public class Singleton {
 
     private static Singleton singleton = null;
 
     private Singleton() {
     }
 
     public static Singleton getInstance() {
         if (singleton == null) {                //1、先验证对象是否创建
 
             synchronized (Singleton.class) {    //2、加锁
 
                 if (singleton == null) {        //3、再次验证对象是否创建
 
                     singleton = new Singleton(); //4、创建对象
                 }
             }
         }
         return singleton;
     }
 }
```

  

上面的代码，我们只需要关心第1步的代码和第4步的代码，在并发情况下，如果线程A和线程B两个同时分别执行1和4，在线程B执行第4步的**过程中**，同时线程A执行到第1步的代码，就可能拿到一个未初始化完成的对象。

上面我重点标记了执行第四步是一个过程，其实我是在重点提示你，第四步的语句不是一个指令，而是由多条指令组成，多条指令执行就会有一个过程。特别是这些指令还可以进行重排序。在进行new Singleton()的时候JVM会生成三个指令。

```
 指令1:  分配对象内存。
 
 指令2:  调用构造器方法，执行初始化。
 
 指令3:  将对象内存引用地址赋值给变量。
```

然后结合到我们上面说的在as-if-serial 原则，这种情况下是可以进行重排序的。 这里很显然指令2和3都依赖于指令1，但是指令3并不依赖于指令2。所以指令3允许被重排序到指令2之前。

```
 指令1:  分配对象内存。
 
 指令3:  将对象内存引用地址赋值给变量。
 
 指令2:  调用构造器方法，执行初始化。
```

好了，这样一看，你就会发现问题所在了，最后执行的流程就可能如下图，最终线程B拿到的对象可能就是一个未初始化完成的对象。所以这里的解决方案就是加一个volatile的关键字，加了volatile关键字后，就禁止了 singleton = new Singleton(); 操作的指令重排序，所以就可以避免问题的出现。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1214-q0ETru.jpg)

  

  

* * *

## **二、解决了缓存可见性问题。**


这里我们还是按照上面的思路来深入扒一下volatile是如何解决缓存可见性问题的。既然是CPU缓存才导致的可见性问题，那么我们就必须要从缓存是如何诞生，然后问题是如何出现的开始说起。

  

  

### **CPU缓存的应用**

为了减少CPU等待IO的时间，让昂贵的CPU资源充分利用起来，提升计算机效率，其中一个思路就是减少IO等待的时间，所以就在CPU的基础上CPU级别的缓存（L1,L2,L3缓存）。  

![1215-stDM0V](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1215-KXyeWE.jpg)

  

  

  

  

在增加了CPU级别的缓存后，当处理器发出内存访问请求时，会先查看高速缓存内是否有请求数据。如果存在（命中），则不需要访问内存直接返回该数据；如果不存在（失效），则要先把内存中的相应数据载入缓存，再将其返回处理器。

  

![1215-zzBrhD](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1215-6MX0o7.jpg)

  

  

### **缓存可见性问题的出现**

而随着CPU的发展，CPU逐渐发展成了多核，CPU可以同时使用多个核心控制器执行线程任务，当然CPU处理同时处理线程任务的速度也越来越快了，但随之也产生了一个问题，多核CPU每个核心控制器工作的时候都会有自己独立的CPU缓存，每个核心控制器都执行任务的时候都是操作的自己的CPU缓存，每个CPU之间的缓存是相互不可见的。

那么这个时候就会出现问题了，比如一个变量a=1，这个变量被CPU-1,CPU-2都缓存了，此时CPU-1执行一个a=a+1的指令，CPU-1就会先修改自己的缓存值为a=a+1=2，再把最新a=2的值同步到主内存中去， 可怜的是CPU-2对已经发生的事情全然不知，还傻傻的用着自己缓存的a=1的值，而这时如果CPU-2也要执行一个a=a+1的指令，那么悲剧就开始发生了。结果就是从业务层面执行了两次a=a+1,最终得到的内存结果a的值却只等于2。 这也就是多核CPU各自缓存不可见导致的问题。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1215-Btdl2Z.jpg)

  

  

### **解决方案一:总线锁**

如果想要避免两个CPU并发修改同一个共享变量，那么最直接的办法就是让这两个CPU操作互斥，当一个CPU准备修改内存数据时，其它CPU只能等待，只有前一个CPU释放了总线锁，那么其它CPU才有机会操作内存。通过这种串行化的方式加上通知机制来保证各个缓存之间的数据一致性，这也就是总线锁的思路。

我们所有内存的传输都发生在一条共享的总线上，同时所有的处理器都能看到这条总线，虽然缓存各自是独立的，但是主存是共享的，所有的内存访问都要经过总线锁，所以可以**通过对总线加锁**来决定CPU是否可以进行内存的读写，也就是说在在同一时刻只可能有一个CPU缓存可以读写内存。

除了总线仲裁机制外，还有一个确保可见性的机制叫“侦听机制”，CPU会侦听发生在总线上的操作，这样当一个CPU通过总线来对数据进行变更的时候其它CPU都会知道，它们以此来使自己的缓存保持同步。只要某个处理器一写内存，其它处理器马上知道这块内存在它们的缓存段中已失效。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1216-zspj3F.jpg)

  

  

### **解决方案二:缓存锁**

因为总线锁是锁定的范围是整个内存，当某个CPU对总线进行加锁之后，其它CPU就无法与主存建立通讯，当一个CPU加锁之后所有后续其它CPU对主存的操作都是阻塞的，所以这种机制性能就必定很差。

很明显，总线锁的弊端在于波及范围太大，需要减小锁定的范围，所以这就是缓存锁的逻辑，既然我只需要改变某一个变量，那么是不是我就只需要锁定内存中的存储了该变量的一小块区域即可？ 的确可以这么做，所以缓存锁的思路就是在CPU在修改共享变量的时候，不锁定整内存，而是只锁定存储了共享变量的缓存行（缓存行为缓存的最小单位），所以缓存锁就极大的整体的性能。

  

  

### **缓存锁实现方式：MESI协议**

缓存锁是一种思路，如何实现那么就需要制定一套机制了，而这就是缓存一致性协议的目标，缓存一致性协议也有多种,比如、MSI、MESI、MOESI，这个是根据不同的操作系统和不同的硬件架构决定的，而且每种协议的具体细节实现细节又不同，其中比较具有代表性的就是MESI。

MESI整体思路就是通过对共享数据进行不同状态的标识，来决定CPU何时把缓存的数据同步到主存，何时可以从缓存读取数据，何时又必须从主存读取数据。MESI 每个字母就代表着一种数据状态，分别是Modified （修改）、Exclusive（独占） 、Share（共享） 、Invalid（失效），每个CPU读取和修改共享数据之前先要识别数据的对象状态，然后根据这几个状态分别执行不同的策略，下面我们分别了解下每个状态所代表的含义。

  

### **Exclusive （独占状态）**

Exclusive代表数据处于独占状态。该状态说明此数据没有其他CPU缓存，只有当前CPU在使用，因为没有其他人使用所以也就肯定当前数据和主存是一致的，所以当CPU发现自己缓存中的共享数据是Exclusive 状态时候就说明该数据是最新值，可以直接读取。

  

![1216-fgrVpH](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1216-gjeEWP.jpg)

  

  

**状态的改变**

当缓存数据处于独Exclusive 态时，一旦后面有其他CPU缓存了该数据，那么当前数据会由Exclusive状态变为Share状态。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1213-yqC47Z.jpg)

  

### **2、Share（共享状态）**

Share 代表数据处于共享状态，此状态代表数据存在于多个缓存之中，而且数据和当前主存中的数据是一致的，这也就意味着当CPU发现自己缓存中的共享数据是Share状态时候就说明该数据是最新值，可以直接读取。

  

![1216-Q19BOL](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1217-Ei66FC.jpg)

  

  

### **3、Modified （修改状态）**

Modified是一个中间状态，处于Modified状态的数据说明该数据正处于修改中，还没有同步到主存去，这个状态下数据可能和主存其它CPU缓存都不一致。

当CPU对缓存数据进行修改时，数据变为Modified状态，并且同时会向其他缓存了该数据的CPU缓存发送一条Invalid指令，告诉其他缓存自己对数据进行了修改，让它们把数据置为Invalid状态； 当收到其它CPU缓存Invalid指令的成功响应时，当前缓存会就会把数据同步到主存里面去，然后自己的数据状态由Modified 变为Exclusive状态，当有其他CPU缓存从主存中读取到了最新的数据时，数据状态会变为Share状态。

  

![1217-fms6DV](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1217-u6G1Af.jpg)

  

  

### **4、Invalid（无效状态）**

一旦有某个CPU对共享的数据进行修改了，那么就会发送一条Invalid指令给其它缓存了相同数据的CPU缓存，收到指令的CPU缓存会把自己的缓存数据状态标记为Invalid，所以当数据处于Invalid时表示数据已经被别人修改了，当前数据是无效的。

  

![1217-UMIpsf](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1217-JDlcq5.jpg)

  

  

MESI协议的核心在于，当一个CPU修改了对应的共享变量时，然后通知其它缓存了对应变量的CPU，收到通知的CPU会把自己的数据标记为invaid无效状态，所有CPU都会根据自己的缓存数据状态来判定自己当前缓存的数据是不是最新的，从而决定是从缓存读取数据还是从内存读取数据。

  

  

### **Store Buffere（MESI性能优化）**

MESI协议中，数据从可用到失效的关键步骤是当一个CPU修改了共享变量的时候，这个时候修改共享变量的CPU需要进行三个操作，修改自己的缓存、通知其它CPU，把最新的数据同步到主内存。而整个过程最耗时的就是通知其它CPU，因为这个过程大必须同步等待所有CPU都响应以后才能继续下面的操作。

  

**比如有两个CPU-1和CPU-2同时缓存了共享变量a=1,此时CPU-1需要修改把共享变量a修改成88，整个流程大致如下：**

  

**刚开始CPU-1和CPU-2都缓存了共享变量a 此时两个CPU的缓存行都处于share状态。**

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1217-UkhNo8.jpg)

  

  

**CPU-1需要修改把共享变量a修改成88，会经过下面几个流程**

1、CPU-1发送invalid通知给CPU-2。

2、CPU-2把自己的缓存a变量的缓存行置为无效状态。

3、响应成功消息给CPU-1。

4、CPU修改自己的缓存行数据a=88，此时缓存行状态变为modified.

5、最后把最新的a=88同步到主内存。

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1213-kB0koR.jpg)

  

  

  

从上面的流程中，我们可以看到，CPU-1通知其它CPU然后得到其他CPU响应的整个过程是同步的，然而有时候其它CPU有可能比较繁忙，并不能马上进行响应，而CPU-1这个过程一直处于等待中，所以很明显这个过程就会降低整个操作的效率。然后就有了对此进行优化的方案。

这里的优化思路也很简单，就是把通知的过程从同步变成了异步。在CPU对某个共享变量修改的时候，向其他CPU发送通知后不会在这里阻塞等待，而是会把最新的值写入到一个Store Buffer里面，然后CPU就可以直接做其它事情了，直到其他CPU都响应成功的通知后，CPU才会把store buffer里最新的数据更新到缓存，最后同步到主内存里。

  

**第一步：向其他CPU发送通知后，把最新的值写入到store buffer，然后CPU操作就算成功了。**

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1218-LKvUqR.jpg)

  

  

**第二步：当其他CPU相应通知后，再把store buffer里的值写入到缓存，最终同步到主存里。**

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1218-DCWbne.jpg)

  

**Store Forward(存储转发)\***

**在上面的流程中，变量写入到store buffer里面后，在没有收到其他CPU响应之前，此时自己缓存的变量值是已经过期的值，所以在这个过程中，如果自己要读该共享变量的值，这里就需要先从Store Buffer里读取，这个过程也叫Store Forward。**

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1218-xo0zBS.jpg)

  

  

### **失效队列（MESI性能优化）**

store buffer优化了共享变量修改操作的性能。不过Store Buffere只是一块很小的空间，当指令过多Store Buffere满了之后，那么新来的指令还是得同步发送指令给其他CPU。 而CPU处理失效消息，标记缓存状态的过程是比较费时的，所以在接收消息的一方加上了一个失效消息队列。当CPU接收到失效消息后，并不会把马上把数据标记为失效，而是把消息放到一个失效队列里，等有时间了才会读取失效队列的消息把数据标记为invalid 状态。

**所以整个流程就会变成如下：**

![](https://cdn.jsdelivr.net/gh/wuwenyishi/shared@image/2022/04/07/1213-mE1Zwb.jpg)

  

  

### **MESI优化带来的问题**

任何优化都是有代价的，这里经过了 store buffer 和invalid queue 优化后性能的确是有了提升，不过随之而来的也面临着一个问题，最初的MESI虽然整个过程是同步进行的，但是这样可以确保每个操作都真正意义上的执行了，从而保证了数据的强一致性。

但是加入了store buffer之后，就使得在修改操作完成后并不能保证缓存和内存的数据得到即时更新。 而在加入invalid queue之后，也使得其它CPU在修改了共享变量之后，并不能即时的把数据标记失效，这就可能造成在某一段时间内，不通过处理器之间还是会存在数据的不一致，整个数据变更的过程变成了弱一致性，而这两个问题就是导致并发问题的根源。

  

  

### **内存屏障(解决MESI优化带来的问题)**

所以针对上面的两个问题需要有针对性的解决方案，这也是内存屏障的来源，操作系统提供了几种类别的内存屏障，

  

### **Store Barrier(写屏障)**

强制所有在store屏障指令之前的store指令，都在该store屏障指令执行之前被执行，并把store缓冲区的数据都刷到CPU缓存。

结合上面的场景，这个指令其实就是告诉CPU，执行这个指令的时候需要把store buffer的数据都同步到内存中去。

  

### **Load Barrier(读屏障)**

强制所有在load屏障指令之后的load指令，都在该load屏障指令执行之后被执行，并且一直等到load缓冲区被该CPU读完才能执行之后的load指令。

这个指令的意思是，在读取共享变量的指令前，先处理所有在失效队列中的消息，这样就保证了在读取数据之前所有失效的消息都得到了执行，从而保证自己是读取到的树是最新的。

  

### **Full Barrier（全能屏障）**

包含了Store Barrier 和Load Barrier的功能。

  

  

### **volatile 和内存屏障**

因为并不是所有程序都需要数据达到实时一致性，如果一个共享变量不存在多个线程并发读取和修改操作时，使用优化过的MESI协议是完全没有问题的，而共享变量是否会被多个线程并发访问和修改只有写程序的人可以判断，所以内存屏障就是提供给软件程序员的一套解决方案，当共享变量可能存在并发问题时，那么软件程序员就需要在对应共享变量的操作上前后加上对应的内存屏障，从而保证程序运行的正确性。

因为不同硬件架构下的内存屏障指令都有所不同，所以在Java为了屏蔽底层系统的复杂性就做了统一的封装，在JAVA里我们不需要知道底层系统那么多的内存屏障指令，只需要在共享变量的定义上加上volatile关键字，volatile关键字定义的共享变量在修改后立马就会暴露给其他CPU，并且能保证在读取volatile定义的共享变量一定是当前最新的数据。其原理就在于操作volatile定义的变量指令前后都会加上内存屏障。

* * *

  

## **总结**


最后总结一下，volatile 定义的变量其实包含三层语义：

1、用volatile定义的共享变量生成的指令不允许进行重排序，从而保证指令的顺序性。

2、在对volatile定义的变量进行修改时，会加上写屏障（或则全能屏障），从而保证修改的共享变量会马上对其他CPU暴露。

3、在对volatile定义变量进行读取的时候，会加上读屏障，从而保证读取的共享变量值是最新的。

  

> 本文转自 [https://zhuanlan.zhihu.com/p/397640787](https://zhuanlan.zhihu.com/p/397640787)，如有侵权，请联系删除。